{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import requests \n",
    "from xml.dom import minidom\n",
    "from ContentDownloader import ContentDownloader\n",
    "from tqdm import tqdm\n",
    "from ipdb import set_trace\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from readability import Document\n",
    "\n",
    "\n",
    "\n",
    "def loadXML(url = 'https://www.salvageautosauction.com/sitemap.xml',name=''): \n",
    "  \n",
    "    # url of rss feed \n",
    "    # creating HTTP response object from given url \n",
    "    resp = requests.get(url) \n",
    "  \n",
    "    # saving the xml file \n",
    "    with open('csv/'+url.split('/')[-1], 'wb') as f: \n",
    "        f.write(resp.content) \n",
    "        \n",
    "        \n",
    "def parseXML(xmlfile): \n",
    "  \n",
    "\n",
    "    xmldoc = minidom.parse(name)\n",
    "    itemlist = xmldoc.getElementsByTagName('loc')\n",
    "    print('URLs found: ',len(itemlist),'\\dProcessing XML file ...')\n",
    "    urls = []\n",
    "    for item in tqdm(itemlist):\n",
    "        url = item.firstChild.nodeValue\n",
    "        if 'vehicle_detail' in url:\n",
    "            urls.append(url)\n",
    "    \n",
    "    return urls \n",
    "\n",
    "\n",
    "def find_bid(page):\n",
    "    try:\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        dom = soup.findAll(\"p\", {\"class\": \"text-center\"})[1]\n",
    "        #return dom.contents[0]\n",
    "        #return ''.join([s for s in dom.contents[0].split() if s.isdigit()])\n",
    "        return int(''.join([s for s in dom.contents[0] if s.isdigit()]))\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "    \n",
    "def format_data(df):\n",
    "    res_df = pd.DataFrame()\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        pagetable = pd.read_html(row['Text'])[0]\n",
    "        item_name = row['URL'].split('vehicle_detail/')[1].split('/')[0]\n",
    "\n",
    "        pagetable.columns = ['Vehicle Name',item_name]\n",
    "        pagetable = pagetable.set_index('Vehicle Name').T\n",
    "        pagetable['bid'] = find_bid(page)\n",
    "\n",
    "        res_df = res_df.append(pagetable,sort=False)\n",
    "\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import requests \n",
    "from xml.dom import minidom\n",
    "from ContentDownloader import ContentDownloader\n",
    "from tqdm import tqdm\n",
    "from ipdb import set_trace\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from readability import Document\n",
    "\n",
    "\n",
    "\n",
    "def loadXML(url = 'https://www.salvageautosauction.com/sitemap.xml',name=''): \n",
    "  \n",
    "    # url of rss feed \n",
    "    # creating HTTP response object from given url \n",
    "    resp = requests.get(url) \n",
    "  \n",
    "    # saving the xml file \n",
    "    with open('csv/'+url.split('/')[-1], 'wb') as f: \n",
    "        f.write(resp.content) \n",
    "        \n",
    "        \n",
    "def parseXML(xmlfile): \n",
    "  \n",
    "\n",
    "    xmldoc = minidom.parse(name)\n",
    "    itemlist = xmldoc.getElementsByTagName('loc')\n",
    "    print('URLs found: ',len(itemlist),'\\dProcessing XML file ...')\n",
    "    urls = []\n",
    "    for item in tqdm(itemlist):\n",
    "        url = item.firstChild.nodeValue\n",
    "        if 'vehicle_detail' in url:\n",
    "            urls.append(url)\n",
    "    \n",
    "    return urls \n",
    "\n",
    "\n",
    "def find_bid(page):\n",
    "    try:\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        dom = soup.findAll(\"p\", {\"class\": \"text-center\"})[1]\n",
    "        #return dom.contents[0]\n",
    "        #return ''.join([s for s in dom.contents[0].split() if s.isdigit()])\n",
    "        return int(''.join([s for s in dom.contents[0] if s.isdigit()]))\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "    \n",
    "def format_data(df):\n",
    "    res_df = pd.DataFrame()\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        pagetable = pd.read_html(row['Text'])[0]\n",
    "        item_name = row['URL'].split('vehicle_detail/')[1].split('/')[0]\n",
    "\n",
    "        pagetable.columns = ['Vehicle Name',item_name]\n",
    "        pagetable = pagetable.set_index('Vehicle Name').T\n",
    "        pagetable['bid'] = find_bid(page)\n",
    "\n",
    "        res_df = res_df.append(pagetable,sort=False)\n",
    "        res_df.to_csv('csv/formated_date.csv',index=False)\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlm_name = 'SAA_sitemap.xml'\n",
    "text_file_name = 'SalvageAutos.csv'\n",
    "loadXML(name=xlm_name)\n",
    "urls = parseXML(xlm_name)\n",
    "\n",
    "scraped_data = ContentDownloader.run_url_download(batch_size=100,urls_list=urls[:300] ,path_to_csv=text_file_name)\n",
    "\n",
    "#scraped_data = pd.read_csv('csv/SalvageAutos.csv')\n",
    "\n",
    "res_df = format_data(scraped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
