{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, sys, os, time , itertools , warnings , re , json\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "# from DataProcessing import \n",
    "import uszipcode , pickle \n",
    "from TempFolder.TempFolder import Temp\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nogit\\\\path','r') as f:\n",
    "    path_to_csv = f.read()\n",
    "path_to_processed_csv =path_to_csv + 'SC-csv\\\\'\n",
    "Temp.set_path(path_to_csv +'SC-temp\\\\')\n",
    "path_to_temp_csv = path_to_csv+'SC-working-folder\\\\'\n",
    "colummn_names = ['Description'\n",
    "                 ,'Title State/Type'\n",
    "                 ,'Location'\n",
    "                 ,'null'\n",
    "                 ,'Auction Date'\n",
    "                 ,'Actual Cash Value'\n",
    "                 ,'Repair Cost'\n",
    "                 ,'Odometer'\n",
    "                 ,'Prim Damage'\n",
    "                 ,'Sec Damage'\n",
    "                 ,'Price Sold or Highest Bid']\n",
    "path_to_makes = glob.glob(path_to_csv+'Cars\\\\*')\n",
    "search = uszipcode.SearchEngine(simple_zipcode=True)\n",
    "path_to_all_years = list(itertools.chain.from_iterable([glob.glob(path+'\\\\*') for path in path_to_makes]))\n",
    "search = uszipcode.SearchEngine(simple_zipcode=True)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #%%time\n",
    "# '''\n",
    "# Parse the HTML files\n",
    "# '''\n",
    "\n",
    "# #\n",
    "# #Read and parse scraped .html files \n",
    "# def process_files(path_to_years__):\n",
    "#     res_df = pd.DataFrame()\n",
    "#     #for each make/year folder get a list of all .html files\n",
    "#     #for path_to__year in tqdm(path_to_years__):\n",
    "#     for path_to__year in path_to_years__:\n",
    "#         try:\n",
    "#             #list of all html\n",
    "#             path_to_make_years = glob.glob(path_to__year+'/*')\n",
    "\n",
    "#             #for each .html in the folder\n",
    "#             for file_path in path_to_make_years:\n",
    "#             #file_path = path_to_make_years[0]\n",
    "            \n",
    "#                 #read file and load to the BS object\n",
    "#                 with open(file_path, 'r') as f:\n",
    "#                     fle = f.read()\n",
    "#                 soup = BeautifulSoup(fle)\n",
    "\n",
    "#                 rows = []\n",
    "#                 line_ind = 0 \n",
    "                \n",
    "#                 #filter all div elements (div table)\n",
    "#                 for div in soup.find_all('div',['row']):\n",
    "#                     row = div.text.strip()\n",
    "#                     #print(row)\n",
    "#                     try:\n",
    "#                         line_dict = {}\n",
    "#                         i = 0\n",
    "                        \n",
    "#                         #clean each element and map to respected column\n",
    "#                         for line in row.replace('\\t','').replace('\\xa0','').split('\\n'):\n",
    "#                             line_dict[colummn_names[i]]= line.replace('Location:','').replace('Title State/Type:','').strip()\n",
    "#                             i+=1\n",
    "#                         rows.append(line_dict)\n",
    "#                     except Exception as e:\n",
    "#                         #print(line_ind,'-',e)\n",
    "#                         pass\n",
    "#                     finally:\n",
    "#                         line_ind+=1\n",
    "\n",
    "#                 page_df = pd.DataFrame(rows).iloc[3:,:-1]\n",
    "#                 page_df_ext = page_df.join(page_df['Description'].str.split(' ',expand=True).rename(columns={0:'Year',1:'Make',2:'Model',3:'Model 2',4:'Model 3'}))\n",
    "#                 page_df_ext = page_df_ext.join(page_df_ext['Location'].str.split('-',expand=True).rename(columns={0:'State',1:'City'}))\n",
    "#                 page_df_ext['City'] = page_df_ext['City'].str.strip().str.capitalize()\n",
    "#                 page_df_ext['State'] = page_df_ext['State'].str.strip()\n",
    "#                 res_df = res_df.append(page_df_ext,ignore_index=True,sort=False)\n",
    "#                 del rows , page_df_ext , page_df\n",
    "#         except:\n",
    "#             continue\n",
    "#     return res_df\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     print('# files to concat' , len(path_to_all_years))\n",
    "#     NCPU = mp.cpu_count()\n",
    "#     print('Creating pool with ', NCPU,' CPUs')\n",
    "#     pool = mp.Pool(processes=4)\n",
    "#     # if not os.path.exists(path_to_processed_csv):\n",
    "#     #     print('Creating output folder')\n",
    "#     #     os.makedirs(path_to_processed_csv)\n",
    "    \n",
    "#     by_chunk_path_to_all_years = np.array_split(path_to_all_years,len(path_to_all_years) // NCPU )\n",
    "#     start_ = 0\n",
    "#     end_ = len(by_chunk_path_to_all_years)\n",
    "#     for i in range(start_,end_):\n",
    "#         print('Starting with chunk: ',i,' of ',end_)\n",
    "#         path_to_all_years_split = np.array_split(by_chunk_path_to_all_years[i],NCPU)\n",
    "#         ress = pool.map(process_files,path_to_all_years_split)\n",
    "#         ress_df = pd.concat(ress)\n",
    "#         print('Chunk ',i,' is done.')\n",
    "#         ress_df.to_csv(path_to_processed_csv+'sc_'+str(i)+'.csv',index=False)\n",
    "# #         i+=1\n",
    "# #         if i ==2:\n",
    "# #             break\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "#     print('all work is done')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read parsed files\n",
    "outputs = glob.glob(path_to_processed_csv+'*')\n",
    "out_l = []\n",
    "errs = []\n",
    "for output in tqdm(outputs):\n",
    "    try:\n",
    "#     print(output)\n",
    "        out_l.append(pd.read_csv(output))\n",
    "    except:\n",
    "        errs.append(output)\n",
    "out_df = pd.concat(out_l,ignore_index=True,sort=False)\n",
    "\n",
    "if len(errs) >0:\n",
    "    print('Errors:', errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out_df.columns)\n",
    "\n",
    "row_size = out_df.shape[0]\n",
    "\n",
    "print('Raw dataset size: ',row_size)\n",
    "\n",
    "\n",
    "out_df.head(50)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up wrong values \n",
    "out_df = out_df.loc[~out_df['Actual Cash Value'].str.contains('Odometer').fillna(False)]\n",
    "out_df = out_df.loc[~out_df['Year'].str.contains('Make').fillna(False)]\n",
    "out_df = out_df.loc[~out_df['Year'].str.contains('Model:').fillna(False)]\n",
    "out_df = out_df.loc[~out_df['Auction Date'].str.contains('Repair Cost').fillna(False)]\n",
    "\n",
    "\n",
    "#filter out canadian cars\n",
    "out_df = out_df.loc[~out_df['Price Sold or Highest Bid'].str.contains('CA').fillna(False)\n",
    "                               ]\n",
    "\n",
    "out_df['Auction Date'] = pd.to_datetime(out_df['Auction Date'])\n",
    "\n",
    "out_df['Auction_Year'] = out_df['Auction Date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_size = out_df.shape[0]\n",
    "print('Cleaned dataset size:',cleaned_size\n",
    "      ,'\\nRows removed: ', row_size - cleaned_size)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # out_df.to_csv(path_to_processed_csv+'SC-countymap.csv',index=False)\n",
    "\n",
    "# # out_df = pd.read_csv(path_to_processed_csv+'SC-countymap.csv')\n",
    "\n",
    "# from DataProcessing.DataStats import get_df_stats\n",
    "\n",
    "# get_df_stats(out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Prim Damages \n",
    "raw_prim_cat = out_df['Prim Damage'].str.lower().unique()\n",
    "print('Cat count:',raw_prim_cat.shape[0],'\\n',raw_prim_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prim_damage_map = {\n",
    " 'burn - engine': 'burn'\n",
    ", 'partial/incomplete r' : 'partial repair'\n",
    ", 'frame damage reporte' : 'frame damage'\n",
    "\n",
    ",'unknown' : 'no data'\n",
    "\n",
    "\n",
    ", np.nan : 'no data'\n",
    ",'price sold or highest bid' :'no data'\n",
    ",'rr' : 'no data'\n",
    "\n",
    "}\n",
    "out_df['Prim Damage'] = out_df['Prim Damage'].str.lower().replace(prim_damage_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prim_cat = out_df['Prim Damage'].str.lower().unique()\n",
    "print('Cat count:',new_prim_cat.shape[0],'\\n',new_prim_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_df = out_df.join(out_df['Location'].str.split('-',expand=True).rename(columns={0:'State',1:'City'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix numerical values\n",
    "def alter_sting(x):    \n",
    "    try:\n",
    "        return float(re.sub('[\\$,USD,\\, ,E,A,N]','',x))\n",
    "    except:\n",
    "#         print(x)\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "out_df['Actual Cash Value'] = (out_df['Actual Cash Value']\n",
    "                               #.fillna(-1)\n",
    "                               .map(alter_sting))\n",
    "out_df['Repair Cost'] = (out_df['Repair Cost']\n",
    "#                          .fillna(-1)\n",
    "                         .map(alter_sting))\n",
    "\n",
    "out_df['Price Sold or Highest Bid'] = (out_df['Price Sold or Highest Bid']\n",
    "                                       #.fillna(-1)\n",
    "                                       .map(alter_sting))\n",
    "out_df['Year'] = (out_df['Year']\n",
    "#                   .fillna(-1)\n",
    "                  .map(float))\n",
    "\n",
    "out_df = out_df.rename(columns={'Year':'Model_Year'\n",
    "                               })\n",
    "                               #.assign(Model_Year = lambda x: int(x))#pd.to_datetime(x['Model_Year']#.astype(int).astype(str))\n",
    "                      \n",
    "\n",
    "out_df['City'] = out_df['City'].str.strip().str.capitalize()\n",
    "out_df['State'] = out_df['State'].str.strip().str.upper()\n",
    "out_df['Model_short'] = out_df['Model'].astype('str')\n",
    "out_df['Model'] = out_df['Model'].astype('str') + ' '+out_df['Model 2'].astype('str').fillna('') + ' ' + out_df['Model 3'].fillna('').astype('str')\n",
    "out_df['Make'] = out_df['Make'].str.strip().str.upper()\n",
    "out_df['Model'] = out_df['Model'].str.strip().str.upper()\n",
    "out_df['Model_short'] = out_df['Model_short'].str.strip().str.upper()\n",
    "\n",
    "#rename variables - add _ where needed\n",
    "\n",
    "out_df = out_df.rename({'Price Sold or Highest Bid':'Price_Sold_or_Highest_Bid'\n",
    "                            ,'Repair Cost':'Repair_Cost'\n",
    "                            ,'Actual Cash Value':'Actual_Cash_Value'\n",
    "                            ,'Prim Damage':'Prim_Damage'\n",
    "                            ,'Sec Damage':'Sec_Damage'\n",
    "                            ,'Title State/Type':'Title_State_Type'\n",
    "                            ,'Auction Date':'Auction_Date'\n",
    "                            },axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# out_df['Auction_Date'] = out_df['Auction_Date'].dt.year\n",
    "out_df['Model_Year'] = out_df['Model_Year'].astype(int)\n",
    "\n",
    "#remove columns \n",
    "\n",
    "out_df = out_df.drop(columns=['Model 2','Model 3', '2', '5'])\n",
    "\n",
    "\n",
    "print('New columns names:',out_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format odometer data\n",
    "\n",
    "#for null model - replace all E  & N values into \"No Data\" (np.nan)\n",
    "\n",
    "def odometer_null(x):\n",
    "    if 'E' in x or 'N' in x:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return alter_sting(x)\n",
    "\n",
    "\n",
    "    \n",
    "#create a dictionary for max E value for each model \n",
    "\n",
    "odometr_dict = (out_df.loc[out_df['Odometer'].str.contains('A')]\n",
    "                 .assign(odometer_num = lambda x: x['Odometer'].map(alter_sting)\n",
    "                        )\n",
    "                 .groupby('Description')['odometer_num']\n",
    "                 .max()\n",
    "                #  [['Odometer','odometer_num']]\n",
    "                )\n",
    "\n",
    "#replace E and N values into max of A category\n",
    "def odometer_replace(x):\n",
    "    if 'E' in x['Odometer'] or 'N' in x['Odometer']:\n",
    "        try:\n",
    "            return odometr_dict[x['Description']]\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return alter_sting(x['Odometer'])\n",
    "\n",
    "\n",
    "#create null model\n",
    "out_df = out_df.assign(Odometer_Null = lambda x: x['Odometer'].map(odometer_null))\n",
    "\n",
    "#create replace model:\n",
    "\n",
    "out_df = out_df.assign(Odometer_Replace = lambda x: x.apply(odometer_replace,axis=1))\n",
    "\n",
    "\n",
    "# create Model_Age\n",
    "# current_year = 2019\n",
    "out_df['Model_Age'] = (out_df['Auction_Year'] - out_df['Model_Year'] ) + 1 #.dt.days // 365 \n",
    "# out_df['Model_Year'] = out_df['Model_Year'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# i = 6\n",
    "# city = out_df.loc[i,'City']\n",
    "# state =out_df.loc[i,'State']\n",
    "# city, state\n",
    "\n",
    "# geo_found = search.by_city_and_state(city,state)[0]\n",
    "\n",
    "# geo_found.county , geo_found.state\n",
    "\n",
    "\n",
    "def find_county(x):\n",
    "    try:\n",
    "        return search.by_city_and_state(x[1] , x[0])[0].county\n",
    "    except:\n",
    "        return 'None'\n",
    "\n",
    "\n",
    "locs = out_df['Location'].unique()\n",
    "locs = locs[~pd.isnull(locs)]\n",
    "\n",
    "state_city = [l[:2] for l in  list(map(lambda x: x.split(' - '),   locs ) ) #if len(l)>1\n",
    "             ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "county = list(map(find_county,   state_city ) )\n",
    "\n",
    "county = [', '.join([l.strip() for l in (county[i].replace('County','')+', '+state_city[i][0]).split(',') ])\n",
    " for i in range(len(county))]\n",
    "\n",
    "loc_lookup = pd.DataFrame({'Location':locs,'County':county}).set_index('Location')['County']\n",
    "\n",
    "loc_lookup\n",
    "\n",
    "\n",
    "def map_countries(x):\n",
    "    try:\n",
    "        return loc_lookup.loc[x]\n",
    "    except:\n",
    "        return 'None'\n",
    "        \n",
    "out_df['County'] = out_df['Location'].map(map_countries)\n",
    "out_df['make_key'] = out_df.assign(Model_Year = lambda x: x['Model_Year'].astype(str))[['Make','Model_short','Model_Year']].agg('_'.join,axis=1)\n",
    "\n",
    "\n",
    "\n",
    "out_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpi adj original data\n",
    "base_year = 2010\n",
    "def cpi_adj_Actual_Cash_Value(x):\n",
    "    try:\n",
    "        \n",
    "        return round((cpi_df[base_year] / cpi_df[x['Auction_Year']]) * x['Actual_Cash_Value'],2)\n",
    "    except:\n",
    "        return x['Actual_Cash_Value']\n",
    "\n",
    "\n",
    "out_df['Actual_Cash_Value_adj'] = out_df.apply(cpi_adj_Actual_Cash_Value\n",
    "                                            ,axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def cpi_adj_Price_Sold_or_Highest_Bid(x):\n",
    "    try:\n",
    "        \n",
    "        return round((cpi_df[base_year] / cpi_df[x['Auction_Year']]) * x['Price_Sold_or_Highest_Bid'],2)\n",
    "    except:\n",
    "        return x['Price_Sold_or_Highest_Bid']\n",
    "\n",
    "\n",
    "out_df['Price_Sold_or_Highest_Bid_adj'] = out_df.apply(cpi_adj_Price_Sold_or_Highest_Bid\n",
    "                                            ,axis=1)\n",
    "\n",
    "def cpi_adj_Repair_Cost(x):\n",
    "    try:\n",
    "        \n",
    "        return round((cpi_df[base_year] / cpi_df[x['Auction_Year']]) * x['Repair_Cost'],2)\n",
    "    except:\n",
    "        return x['Repair_Cost']\n",
    "\n",
    "out_df['Repair_Cost_adj'] = out_df.apply(cpi_adj_Repair_Cost\n",
    "                                            ,axis=1)\n",
    "\n",
    "out_df['make_key'] = out_df.assign(Model_Year = lambda x: x['Model_Year'].astype(str))[['Make','Model_short','Model_Year']].agg('_'.join,axis=1)\n",
    "\n",
    "#out_df['Make'].value_counts().to_dict()#.to_string()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Object <out_df_working> loaded from: c:\\data\\Datasets\\SC-temp\\\n"
    }
   ],
   "source": [
    "# Temp.save_obj(out_df, 'out_df_working')\n",
    "\n",
    "out_df = Temp.load_obj('out_df_working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\nEND OF MAIN DATASET PREPROCESSING\\n'"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "'''\n",
    "END OF MAIN DATASET PREPROCESSING\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Demographic features:\n ['index', 'Unemp-Pct-2014', 'Unemp-Pct-2015', 'Unemp-Pct-2016', 'Unemp-Pct-2017', 'Unemp-Pct-2018', 'Unemp-Pct-2013', 'Unemp-Pct-2012', 'Unemp-Pct-2011', 'Unemp-Pct-2010', 'Income_adj-1990', 'Income_adj-1991', 'Income_adj-1992', 'Income_adj-1993', 'Income_adj-1994', 'Income_adj-1995', 'Income_adj-1996', 'Income_adj-1997', 'Income_adj-1998', 'Income_adj-1999', 'Income_adj-2000', 'Income_adj-2001', 'Income_adj-2002', 'Income_adj-2003', 'Income_adj-2004', 'Income_adj-2005', 'Income_adj-2006', 'Income_adj-2007', 'Income_adj-2008', 'Income_adj-2009', 'Income_adj-2010', 'Income_adj-2011', 'Income_adj-2012', 'Income_adj-2013', 'Income_adj-2014', 'Income_adj-2015', 'Income_adj-2016', 'Income_adj-2017', 'CENSUS2010', 'CENSUS2011', 'CENSUS2012', 'CENSUS2013', 'CENSUS2014', 'CENSUS2015', 'CENSUS2016', 'CENSUS2017', 'CENSUS2018', 'GRNDTOT_2009', 'GRNDTOT_2010', 'GRNDTOT_2011', 'GRNDTOT_2012', 'GRNDTOT_2013', 'GRNDTOT_2014', 'GRNDTOT_2016', 'CPOPARST_2009', 'CPOPARST_2010', 'CPOPARST_2011', 'CPOPARST_2012', 'CPOPARST_2013', 'CPOPARST_2014', 'CPOPARST_2016']\n"
    }
   ],
   "source": [
    "'''\n",
    "JOINING WITH DEMOGRAPHIC DATA \n",
    "'''\n",
    "\n",
    "\n",
    "#read the demography data `\n",
    "\n",
    "path_to_demograpyh =path_to_csv+'SC-working-folder\\\\demography.csv'\n",
    "\n",
    "demog = pd.read_csv(path_to_demograpyh)\n",
    "\n",
    "print('Demographic features:\\n' , demog.columns.tolist())\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "    make model  year  Original_MSRP_low  Original_MSRP_high  \\\n0  ACURA    CL  2001            28460.0             30810.0   \n1  ACURA    CL  2002            28530.0             32880.0   \n2  ACURA    CL  2003            28700.0             31050.0   \n3  ACURA   ILX  2013            25900.0             34400.0   \n4  ACURA   ILX  2014            26900.0             34600.0   \n\n   Original_MSRP_mean  Original_MSRP_mean_adj        make_key  \n0             29635.0                 29635.0   ACURA_CL_2001  \n1             30705.0                 30705.0   ACURA_CL_2002  \n2             29875.0                 29875.0   ACURA_CL_2003  \n3             30150.0                 30150.0  ACURA_ILX_2013  \n4             30750.0                 30750.0  ACURA_ILX_2014  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>make</th>\n      <th>model</th>\n      <th>year</th>\n      <th>Original_MSRP_low</th>\n      <th>Original_MSRP_high</th>\n      <th>Original_MSRP_mean</th>\n      <th>Original_MSRP_mean_adj</th>\n      <th>make_key</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ACURA</td>\n      <td>CL</td>\n      <td>2001</td>\n      <td>28460.0</td>\n      <td>30810.0</td>\n      <td>29635.0</td>\n      <td>29635.0</td>\n      <td>ACURA_CL_2001</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ACURA</td>\n      <td>CL</td>\n      <td>2002</td>\n      <td>28530.0</td>\n      <td>32880.0</td>\n      <td>30705.0</td>\n      <td>30705.0</td>\n      <td>ACURA_CL_2002</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ACURA</td>\n      <td>CL</td>\n      <td>2003</td>\n      <td>28700.0</td>\n      <td>31050.0</td>\n      <td>29875.0</td>\n      <td>29875.0</td>\n      <td>ACURA_CL_2003</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ACURA</td>\n      <td>ILX</td>\n      <td>2013</td>\n      <td>25900.0</td>\n      <td>34400.0</td>\n      <td>30150.0</td>\n      <td>30150.0</td>\n      <td>ACURA_ILX_2013</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ACURA</td>\n      <td>ILX</td>\n      <td>2014</td>\n      <td>26900.0</td>\n      <td>34600.0</td>\n      <td>30750.0</td>\n      <td>30750.0</td>\n      <td>ACURA_ILX_2014</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "\n",
    "#read carfax data\n",
    "path_to_carfax =path_to_csv+'SC-carfax\\\\processed\\\\MSRP.csv'\n",
    "\n",
    "carfax = pd.read_csv(path_to_carfax,index_col=0) \n",
    "\n",
    "#CPI Data\n",
    "path_to_cpi = path_to_csv+'US-CPI\\\\SeriesReport-20191118124049_549fff.xlsx'\n",
    "cpi_df = pd.read_excel(path_to_cpi,skiprows=11,index_col=0)['Annual'][:-2]; cpi_df\n",
    "\n",
    "\n",
    "#read carfax data\n",
    "path_to_carfax =path_to_csv+'SC-carfax\\\\processed\\\\MSRP.csv'\n",
    "\n",
    "carfax = pd.read_csv(path_to_carfax,index_col=0) \n",
    "carfax['make'] = carfax['make'].str.upper()\n",
    "carfax['model'] = carfax['model'].str.upper()\n",
    "carfax['year'] = carfax['year'].astype(str)\n",
    "\n",
    "carfax['make_key'] = carfax[['make','model','year']].agg('_'.join,axis=1)\n",
    "\n",
    "\n",
    "carfax['Original_MSRP_mean'] =carfax[['Original_MSRP_low','Original_MSRP_high']].mean(axis=1)\n",
    "\n",
    "#CPI adjusted income \n",
    "def cpi_adj_Original_MSRP_mean(x):\n",
    "    try:\n",
    "        \n",
    "        return round((cpi_df[base_year] / cpi_df[x['year']]) * x['Original_MSRP_mean'],2)\n",
    "    except:\n",
    "        return x['Original_MSRP_mean']\n",
    "\n",
    "\n",
    "carfax['Original_MSRP_mean_adj'] = carfax.apply(cpi_adj_Original_MSRP_mean\n",
    "                                            ,axis=1)\n",
    "\n",
    "\n",
    "#carfax['make'].value_counts().to_dict()#.to_string()\n",
    "carfax.head()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Actual_Cash_Value Auction_Date               Description  \\\n0                NaN   2019-10-21  2004 Oldsmobile ALERO GL   \n1             1304.0   2019-10-14  2004 Oldsmobile ALERO GL   \n2             2550.0   2019-09-30  2004 Oldsmobile ALERO GL   \n3             2325.0   2019-09-19  2004 Oldsmobile ALERO GL   \n4             2107.0   2018-07-17  2004 Oldsmobile ALERO GL   \n\n              Location  Odometer  Price_Sold_or_Highest_Bid  Prim_Damage  \\\n0       MD - BALTIMORE  151441 A                      550.0  normal wear   \n1       TN - NASHVILLE  213655 E                      800.0    front end   \n2         AZ - PHOENIX       0 N                      400.0         burn   \n3  CA - SAN BERNARDINO  241435 A                      200.0    front end   \n4     LA - BATON ROUGE       0 E                      250.0    front end   \n\n   Repair_Cost            Sec_Damage Title_State_Type  ...  \\\n0          NaN  Minor dent/scratches            MD CT  ...   \n1          NaN              Rear end            TN CT  ...   \n2          NaN  Minor dent/scratches            AZ CT  ...   \n3       5712.0                  Side            CA SC  ...   \n4       2107.0                   NaN            LA ST  ...   \n\n   Actual_Cash_Value_adj Price_Sold_or_Highest_Bid_adj Repair_Cost_adj  \\\n0                    NaN                         550.0             NaN   \n1                 1304.0                         800.0             NaN   \n2                 2550.0                         400.0             NaN   \n3                 2325.0                         200.0          5712.0   \n4                 2107.0                         250.0          2107.0   \n\n         make  model  year Original_MSRP_low  Original_MSRP_high  \\\n0  OLDSMOBILE  ALERO  2004           19110.0             23960.0   \n1  OLDSMOBILE  ALERO  2004           19110.0             23960.0   \n2  OLDSMOBILE  ALERO  2004           19110.0             23960.0   \n3  OLDSMOBILE  ALERO  2004           19110.0             23960.0   \n4  OLDSMOBILE  ALERO  2004           19110.0             23960.0   \n\n   Original_MSRP_mean  Original_MSRP_mean_adj  \n0             21535.0                 21535.0  \n1             21535.0                 21535.0  \n2             21535.0                 21535.0  \n3             21535.0                 21535.0  \n4             21535.0                 21535.0  \n\n[5 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Actual_Cash_Value</th>\n      <th>Auction_Date</th>\n      <th>Description</th>\n      <th>Location</th>\n      <th>Odometer</th>\n      <th>Price_Sold_or_Highest_Bid</th>\n      <th>Prim_Damage</th>\n      <th>Repair_Cost</th>\n      <th>Sec_Damage</th>\n      <th>Title_State_Type</th>\n      <th>...</th>\n      <th>Actual_Cash_Value_adj</th>\n      <th>Price_Sold_or_Highest_Bid_adj</th>\n      <th>Repair_Cost_adj</th>\n      <th>make</th>\n      <th>model</th>\n      <th>year</th>\n      <th>Original_MSRP_low</th>\n      <th>Original_MSRP_high</th>\n      <th>Original_MSRP_mean</th>\n      <th>Original_MSRP_mean_adj</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>2019-10-21</td>\n      <td>2004 Oldsmobile ALERO GL</td>\n      <td>MD - BALTIMORE</td>\n      <td>151441 A</td>\n      <td>550.0</td>\n      <td>normal wear</td>\n      <td>NaN</td>\n      <td>Minor dent/scratches</td>\n      <td>MD CT</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>550.0</td>\n      <td>NaN</td>\n      <td>OLDSMOBILE</td>\n      <td>ALERO</td>\n      <td>2004</td>\n      <td>19110.0</td>\n      <td>23960.0</td>\n      <td>21535.0</td>\n      <td>21535.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1304.0</td>\n      <td>2019-10-14</td>\n      <td>2004 Oldsmobile ALERO GL</td>\n      <td>TN - NASHVILLE</td>\n      <td>213655 E</td>\n      <td>800.0</td>\n      <td>front end</td>\n      <td>NaN</td>\n      <td>Rear end</td>\n      <td>TN CT</td>\n      <td>...</td>\n      <td>1304.0</td>\n      <td>800.0</td>\n      <td>NaN</td>\n      <td>OLDSMOBILE</td>\n      <td>ALERO</td>\n      <td>2004</td>\n      <td>19110.0</td>\n      <td>23960.0</td>\n      <td>21535.0</td>\n      <td>21535.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2550.0</td>\n      <td>2019-09-30</td>\n      <td>2004 Oldsmobile ALERO GL</td>\n      <td>AZ - PHOENIX</td>\n      <td>0 N</td>\n      <td>400.0</td>\n      <td>burn</td>\n      <td>NaN</td>\n      <td>Minor dent/scratches</td>\n      <td>AZ CT</td>\n      <td>...</td>\n      <td>2550.0</td>\n      <td>400.0</td>\n      <td>NaN</td>\n      <td>OLDSMOBILE</td>\n      <td>ALERO</td>\n      <td>2004</td>\n      <td>19110.0</td>\n      <td>23960.0</td>\n      <td>21535.0</td>\n      <td>21535.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2325.0</td>\n      <td>2019-09-19</td>\n      <td>2004 Oldsmobile ALERO GL</td>\n      <td>CA - SAN BERNARDINO</td>\n      <td>241435 A</td>\n      <td>200.0</td>\n      <td>front end</td>\n      <td>5712.0</td>\n      <td>Side</td>\n      <td>CA SC</td>\n      <td>...</td>\n      <td>2325.0</td>\n      <td>200.0</td>\n      <td>5712.0</td>\n      <td>OLDSMOBILE</td>\n      <td>ALERO</td>\n      <td>2004</td>\n      <td>19110.0</td>\n      <td>23960.0</td>\n      <td>21535.0</td>\n      <td>21535.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2107.0</td>\n      <td>2018-07-17</td>\n      <td>2004 Oldsmobile ALERO GL</td>\n      <td>LA - BATON ROUGE</td>\n      <td>0 E</td>\n      <td>250.0</td>\n      <td>front end</td>\n      <td>2107.0</td>\n      <td>NaN</td>\n      <td>LA ST</td>\n      <td>...</td>\n      <td>2107.0</td>\n      <td>250.0</td>\n      <td>2107.0</td>\n      <td>OLDSMOBILE</td>\n      <td>ALERO</td>\n      <td>2004</td>\n      <td>19110.0</td>\n      <td>23960.0</td>\n      <td>21535.0</td>\n      <td>21535.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# join original and carfex\n",
    "out_df_carfix = pd.merge(out_df, carfax,  how='inner', on ='make_key').drop(columns='make_key')\n",
    "#out_df_carfix['County']\n",
    "\n",
    "out_df_carfix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(516218, 82)    Actual_Cash_Value Auction_Date  Price_Sold_or_Highest_Bid  Prim_Damage  \\\n0                NaN   2019-10-21                      550.0  normal wear   \n1             1304.0   2019-10-14                      800.0    front end   \n2             2550.0   2019-09-30                      400.0         burn   \n3             2325.0   2019-09-19                      200.0    front end   \n4             2107.0   2018-07-17                      250.0    front end   \n\n   Repair_Cost  Model_Year        Make     Model  Auction_Year Model_short  \\\n0          NaN        2004  OLDSMOBILE  ALERO GL          2019       ALERO   \n1          NaN        2004  OLDSMOBILE  ALERO GL          2019       ALERO   \n2          NaN        2004  OLDSMOBILE  ALERO GL          2019       ALERO   \n3       5712.0        2004  OLDSMOBILE  ALERO GL          2019       ALERO   \n4       2107.0        2004  OLDSMOBILE  ALERO GL          2018       ALERO   \n\n   ...  GRNDTOT_2013  GRNDTOT_2014  GRNDTOT_2016 CPOPARST_2009  CPOPARST_2010  \\\n0  ...           NaN           NaN           NaN           NaN            NaN   \n1  ...       54030.0       50131.0       45438.0      628812.0       621332.0   \n2  ...      168892.0      154617.0      136056.0     4017355.0      3816830.0   \n3  ...           NaN           NaN           NaN           NaN            NaN   \n4  ...           NaN           NaN           NaN           NaN            NaN   \n\n   CPOPARST_2011  CPOPARST_2012  CPOPARST_2013  CPOPARST_2014  CPOPARST_2016  \n0            NaN            NaN            NaN            NaN            NaN  \n1       626937.0       635244.0       649979.0       662275.0       687373.0  \n2      3870864.0      3922299.0      3997329.0      4086811.0      4248828.0  \n3            NaN            NaN            NaN            NaN            NaN  \n4            NaN            NaN            NaN            NaN            NaN  \n\n[5 rows x 82 columns] \n\n Index(['Actual_Cash_Value', 'Auction_Date', 'Price_Sold_or_Highest_Bid',\n       'Prim_Damage', 'Repair_Cost', 'Model_Year', 'Make', 'Model',\n       'Auction_Year', 'Model_short', 'Odometer_Null', 'Odometer_Replace',\n       'Model_Age', 'County', 'Actual_Cash_Value_adj',\n       'Price_Sold_or_Highest_Bid_adj', 'Repair_Cost_adj', 'Original_MSRP_low',\n       'Original_MSRP_high', 'Original_MSRP_mean', 'Original_MSRP_mean_adj',\n       'index', 'Unemp-Pct-2014', 'Unemp-Pct-2015', 'Unemp-Pct-2016',\n       'Unemp-Pct-2017', 'Unemp-Pct-2018', 'Unemp-Pct-2013', 'Unemp-Pct-2012',\n       'Unemp-Pct-2011', 'Unemp-Pct-2010', 'Income_adj-1990',\n       'Income_adj-1991', 'Income_adj-1992', 'Income_adj-1993',\n       'Income_adj-1994', 'Income_adj-1995', 'Income_adj-1996',\n       'Income_adj-1997', 'Income_adj-1998', 'Income_adj-1999',\n       'Income_adj-2000', 'Income_adj-2001', 'Income_adj-2002',\n       'Income_adj-2003', 'Income_adj-2004', 'Income_adj-2005',\n       'Income_adj-2006', 'Income_adj-2007', 'Income_adj-2008',\n       'Income_adj-2009', 'Income_adj-2010', 'Income_adj-2011',\n       'Income_adj-2012', 'Income_adj-2013', 'Income_adj-2014',\n       'Income_adj-2015', 'Income_adj-2016', 'Income_adj-2017', 'CENSUS2010',\n       'CENSUS2011', 'CENSUS2012', 'CENSUS2013', 'CENSUS2014', 'CENSUS2015',\n       'CENSUS2016', 'CENSUS2017', 'CENSUS2018', 'GRNDTOT_2009',\n       'GRNDTOT_2010', 'GRNDTOT_2011', 'GRNDTOT_2012', 'GRNDTOT_2013',\n       'GRNDTOT_2014', 'GRNDTOT_2016', 'CPOPARST_2009', 'CPOPARST_2010',\n       'CPOPARST_2011', 'CPOPARST_2012', 'CPOPARST_2013', 'CPOPARST_2014',\n       'CPOPARST_2016'],\n      dtype='object')\n"
    }
   ],
   "source": [
    "# with open('var/final_columns.json') as f:\n",
    "#     final_columns = json.load(f)['final_columns']\n",
    "\n",
    "car_demo_joined = (pd.merge(out_df_carfix, demog,left_on='County',right_on='index',how='left')\n",
    "                    .drop(columns=['Location','Description','Odometer','Sec_Damage','Title_State_Type','State', 'City','make', 'model','year'])\t\t\t\t\t\n",
    "                    )\n",
    "\t\t\t\t\n",
    "\n",
    "\n",
    "print(car_demo_joined.shape, car_demo_joined.head(),'\\n\\n' ,car_demo_joined.columns)\n",
    "\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "working with  ['Unemp-Pct-2014', 'Unemp-Pct-2015', 'Unemp-Pct-2016', 'Unemp-Pct-2017', 'Unemp-Pct-2018', 'Unemp-Pct-2013', 'Unemp-Pct-2012', 'Unemp-Pct-2011', 'Unemp-Pct-2010']\n"
    }
   ],
   "source": [
    "#create master demographic field \n",
    "\n",
    "# 1 Unemp-Pct\n",
    "\n",
    "unemp_columns  = [col for col in car_demo_joined.columns if 'Unemp-Pct' in col]\n",
    "print('working with ' ,unemp_columns)\n",
    "def fetch_unemp_null(x):\n",
    "    ''' Return the actual on the action date unemployment, if none, return None'''\n",
    "    try:\n",
    "        auction_y = str(x['Auction_Year'])\n",
    "        return x[f'Unemp-Pct-{auction_y}']\n",
    "    except:\n",
    "        return\n",
    "\n",
    "car_demo_joined['Unemp_pct_null'] = car_demo_joined.apply(fetch_unemp_null, axis=1)\n",
    "\n",
    "unemp_available = np.array([2014,2015,2016,2017,2018,2013,2012,2011,2010])\n",
    "def fetch_unemp_closest(x):\n",
    "    ''' Return the actual on the action date unemployment, if none, return abs closest'''\n",
    "    try:\n",
    "        auction_y = str(x['Auction_Year'])\n",
    "        return x[f'Unemp-Pct-{auction_y}']\n",
    "    except:\n",
    "        closest_year_ix = np.abs( unemp_available - x['Auction_Year']).argmin()\n",
    "        closest_year = unemp_available[closest_year_ix]\n",
    "        return x[f'Unemp-Pct-{closest_year}']\n",
    "\n",
    "car_demo_joined['Unemp_pct_closest'] = car_demo_joined.apply(fetch_unemp_closest, axis=1)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "working with  ['Income_adj-1990', 'Income_adj-1991', 'Income_adj-1992', 'Income_adj-1993', 'Income_adj-1994', 'Income_adj-1995', 'Income_adj-1996', 'Income_adj-1997', 'Income_adj-1998', 'Income_adj-1999', 'Income_adj-2000', 'Income_adj-2001', 'Income_adj-2002', 'Income_adj-2003', 'Income_adj-2004', 'Income_adj-2005', 'Income_adj-2006', 'Income_adj-2007', 'Income_adj-2008', 'Income_adj-2009', 'Income_adj-2010', 'Income_adj-2011', 'Income_adj-2012', 'Income_adj-2013', 'Income_adj-2014', 'Income_adj-2015', 'Income_adj-2016', 'Income_adj-2017']\n"
    }
   ],
   "source": [
    "# 2 Income_adj\n",
    "inc_columns  = [col for col in car_demo_joined.columns if 'Income_adj' in col]\n",
    "print('working with ' ,inc_columns)\n",
    "\n",
    "def fetch_inc_null(x):\n",
    "    ''' Return the actual on the action date income, if none, return None'''\n",
    "    try:\n",
    "        auction_y = str(x['Auction_Year'])\n",
    "        return x[f'Income_adj-{auction_y}']\n",
    "    except:\n",
    "        return\n",
    "\n",
    "car_demo_joined['Income_adj_null'] = car_demo_joined.apply(fetch_inc_null, axis=1)\n",
    "\n",
    "\n",
    "income_available = np.arange(1990,2018)\n",
    "def fetch_inc_closest(x):\n",
    "    ''' Return the actual on the action date income, if none, return None'''\n",
    "    try:\n",
    "        auction_y = str(x['Auction_Year'])\n",
    "        return x[f'Income_adj-{auction_y}']\n",
    "    except:\n",
    "        closest_year_ix = np.abs( income_available - x['Auction_Year']).argmin()\n",
    "        closest_year = income_available[closest_year_ix]\n",
    "        return x[f'Income_adj-{closest_year}']\n",
    "\n",
    "car_demo_joined['Income_adj_closest'] = car_demo_joined.apply(fetch_inc_closest, axis=1)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "working with  ['CENSUS2010', 'CENSUS2011', 'CENSUS2012', 'CENSUS2013', 'CENSUS2014', 'CENSUS2015', 'CENSUS2016', 'CENSUS2017', 'CENSUS2018']\n"
    }
   ],
   "source": [
    "# 3 CENSUS\n",
    "CENSUS_columns  = [col for col in car_demo_joined.columns if 'CENSUS' in col]\n",
    "print('working with ' ,CENSUS_columns)\n",
    "\n",
    "def CENSUS_null(x):\n",
    "    ''' Return the actual on the action date CENSUS, if none, return None'''\n",
    "    try:\n",
    "        auction_y = str(x['Auction_Year'])\n",
    "        return x[f'CENSUS{auction_y}']\n",
    "    except:\n",
    "        return\n",
    "\n",
    "car_demo_joined['CENSUS_null'] = car_demo_joined.apply(CENSUS_null, axis=1)\n",
    "\n",
    "\n",
    "CENSUS_available = np.arange(2010,2019)\n",
    "def CENSUS_closest(x):\n",
    "    ''' Return the actual on the action date CENSUS, if none, return None'''\n",
    "    try:\n",
    "        auction_y = str(x['Auction_Year'])\n",
    "        return x[f'CENSUS{auction_y}']\n",
    "    except:\n",
    "        closest_year_ix = np.abs( CENSUS_available - x['Auction_Year']).argmin()\n",
    "        closest_year = CENSUS_available[closest_year_ix]\n",
    "        return x[f'CENSUS{closest_year}']\n",
    "\n",
    "car_demo_joined['CENSUS_closest'] = car_demo_joined.apply(CENSUS_closest, axis=1)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "working with  ['GRNDTOT_2009', 'GRNDTOT_2010', 'GRNDTOT_2011', 'GRNDTOT_2012', 'GRNDTOT_2013', 'GRNDTOT_2014', 'GRNDTOT_2016']\n"
    }
   ],
   "source": [
    "# 4 - GRNDTOT\n",
    "GRNDTOT_columns  = [col for col in car_demo_joined.columns if 'GRNDTOT' in col]\n",
    "print('working with ' ,GRNDTOT_columns)\n",
    "\n",
    "def GRNDTOT_null(x):\n",
    "    ''' Return the actual on the action date GRNDTOT, if none, return None'''\n",
    "    try:\n",
    "        auction_y = str(x['Auction_Year'])\n",
    "        return x[f'GRNDTOT_{auction_y}']\n",
    "    except:\n",
    "        return\n",
    "\n",
    "car_demo_joined['GRNDTOT_null'] = car_demo_joined.apply(GRNDTOT_null, axis=1)\n",
    "\n",
    "\n",
    "GRNDTOT_available = np.array([2009,2010,2011,2012,2013,2014,2016])\n",
    "def GRNDTOT_closest(x):\n",
    "    ''' Return the actual on the action date GRNDTOT, if none, return None'''\n",
    "    try:\n",
    "        auction_y = str(x['Auction_Year'])\n",
    "        return x[f'GRNDTOT_{auction_y}']\n",
    "    except:\n",
    "        closest_year_ix = np.abs( GRNDTOT_available - x['Auction_Year']).argmin()\n",
    "        closest_year = GRNDTOT_available[closest_year_ix]\n",
    "        return x[f'GRNDTOT_{closest_year}']\n",
    "\n",
    "car_demo_joined['GRNDTOT_closest'] = car_demo_joined.apply(GRNDTOT_closest, axis=1)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "working with  ['GRNDTOT_2009', 'GRNDTOT_2010', 'GRNDTOT_2011', 'GRNDTOT_2012', 'GRNDTOT_2013', 'GRNDTOT_2014', 'GRNDTOT_2016']\n"
    }
   ],
   "source": [
    "# 5 - CPOPARST\n",
    "CPOPARST_columns  = [col for col in car_demo_joined.columns if 'CPOPARST' in col]\n",
    "print('working with ' ,GRNDTOT_columns)\n",
    "\n",
    "def CPOPARST_null(x):\n",
    "    ''' Return the actual on the action date CPOPARST, if none, return None'''\n",
    "    try:\n",
    "        auction_y = str(x['Auction_Year'])\n",
    "        return x[f'CPOPARST_{auction_y}']\n",
    "    except:\n",
    "        return\n",
    "\n",
    "car_demo_joined['CPOPARST_null'] = car_demo_joined.apply(CPOPARST_null, axis=1)\n",
    "\n",
    "\n",
    "CPOPARST_available = np.array([2009,2010,2011,2012,2013,2014,2016])\n",
    "def CPOPARST_closest(x):\n",
    "    ''' Return the actual on the action date CPOPARST, if none, return None'''\n",
    "    try:\n",
    "        auction_y = str(x['Auction_Year'])\n",
    "        return x[f'CPOPARST_{auction_y}']\n",
    "    except:\n",
    "        closest_year_ix = np.abs( CPOPARST_available - x['Auction_Year']).argmin()\n",
    "        closest_year = CPOPARST_available[closest_year_ix]\n",
    "        return x[f'CPOPARST_{closest_year}']\n",
    "\n",
    "car_demo_joined['CPOPARST_closest'] = car_demo_joined.apply(CPOPARST_closest, axis=1)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove yearly demographic data \n",
    "col_to_rem = [unemp_columns,inc_columns ,CENSUS_columns,GRNDTOT_columns,CPOPARST_columns]\n",
    "from itertools import chain\n",
    "col_to_rem_fl = list(chain.from_iterable(col_to_rem))\n",
    "#col_to_rem#[np.array(i) for i in col_to_rem]\n",
    "\n",
    "car_demo_joined = car_demo_joined.drop(columns=col_to_rem_fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'Actual_Cash_Value': 1304.0,\n 'Auction_Date': Timestamp('2019-10-14 00:00:00'),\n 'Price_Sold_or_Highest_Bid': 800.0,\n 'Prim_Damage': 'front end',\n 'Repair_Cost': nan,\n 'Model_Year': 2004,\n 'Make': 'OLDSMOBILE',\n 'Model': 'ALERO GL',\n 'Auction_Year': 2019,\n 'Model_short': 'ALERO',\n 'Odometer_Null': nan,\n 'Odometer_Replace': 297038.0,\n 'Model_Age': 16,\n 'County': 'Davidson, TN',\n 'Actual_Cash_Value_adj': 1304.0,\n 'Price_Sold_or_Highest_Bid_adj': 800.0,\n 'Repair_Cost_adj': nan,\n 'Original_MSRP_low': 19110.0,\n 'Original_MSRP_high': 23960.0,\n 'Original_MSRP_mean': 21535.0,\n 'Original_MSRP_mean_adj': 21535.0,\n 'index': 'Davidson, TN',\n 'Unemp_pct_null': nan,\n 'Unemp_pct_closest': 2.6,\n 'Income_adj_null': nan,\n 'Income_adj_closest': 56100.136781984336,\n 'CENSUS_null': nan,\n 'CENSUS_closest': 692587.0,\n 'GRNDTOT_null': nan,\n 'GRNDTOT_closest': 45438.0,\n 'CPOPARST_null': nan,\n 'CPOPARST_closest': 687373.0}"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "car_demo_joined.loc[1].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_demo_joined.to_csv(path_to_temp_csv+'joint_working_df.csv')\n",
    "Temp.save_obj(car_demo_joined,'car_demo_joined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ANALYSIS\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del out_df\n",
    "out_df = Temp.load_obj('out_df_working')\n",
    "# out_df = Temp.load_obj('car_demo_joined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.groupby('Make')['Model_Year'].count().sort_values(ascending=False)[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore outlyers for \n",
    "var_to_plot ='Model_Age'\n",
    "fig, ax = plt.subplots(1,2,figsize=(7,5))\n",
    "out_df[var_to_plot].plot.box(ax=ax[0])\n",
    "out_df[var_to_plot].plot.box(ax=ax[1],showfliers =False)\n",
    "\n",
    "\n",
    "fig.suptitle(var_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filterd_acv = out_df[var_to_plot]\n",
    "# filterd_acv_round = filterd_acv\n",
    "\n",
    "\n",
    "# filterd_acv_round_vc = filterd_acv_round.value_counts().to_frame().reset_index().rename({'index':'level' , var_to_plot:'count'}, axis=1)\n",
    "# filterd_acv_round_vc.to_csv('csv/outly.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram\n",
    "\n",
    "s = 20\n",
    "e = 80\n",
    "#display modes of the distribution \n",
    "try:\n",
    "    filterd_acv = out_df[var_to_plot].loc[(out_df[var_to_plot] >s )& (out_df[var_to_plot] <e)]\n",
    "\n",
    "    print('Counts:\\n#>100k:'\n",
    "                ,len(out_df[var_to_plot].loc[out_df[var_to_plot]>=1e5]))\n",
    "\n",
    "except:\n",
    "    filterd_acv = out_df[var_to_plot]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n = 1\n",
    "filterd_acv_round = (filterd_acv // n ) * n\n",
    "\n",
    "filterd_acv.plot.hist(title= var_to_plot+f' (between {s} and {e})',color='k', alpha=.4)\n",
    "\n",
    "filterd_acv_round_vc = filterd_acv_round.value_counts().to_frame().reset_index().rename({'index':'level' , var_to_plot:'count'}, axis=1)\n",
    "filterd_acv_round_vc.to_csv('csv/outly.csv', index=False)\n",
    "# filterd_acv_round_vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#plot a histogram of observed cash_values\n",
    "fig= plt.figure()\n",
    "ax = filterd_acv.plot.hist()\n",
    "ax.set(title = 'Histogram for Actual_Cash_Value < 10k');\n",
    "\n",
    "\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit  normat distribution\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "x = np.linspace(0,filterd_acv.max(),len(filterd_acv))\n",
    "mu , std = stats.norm.fit(filterd_acv.values)\n",
    "\n",
    "ax = filterd_acv.plot.hist(normed=True)\n",
    "plt.plot(x, stats.norm.pdf(x , mu, std))\n",
    "ax.set(title = 'Histogram for Actual_Cash_Value < 10k');\n",
    "print('K-S p-value:', stats.kstest(filterd_acv.values , 'norm')[1])\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_df['Model_Age'].describe().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_age\n",
    "#check model age statsÐ’\n",
    "\n",
    "out_df.loc[out_df['Model_Age'].loc[out_df['Model_Age']<0].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numerical_fields =  ['Actual_Cash_Value'\n",
    "               ,'Auction_Year'\n",
    "       #'Odometer'\n",
    "       , 'Price_Sold_or_Highest_Bid'\n",
    "       , 'Repair_Cost'\n",
    "        ,'Odometer_Null','Odometer_Replace'\n",
    "        ,'Model_Age'\n",
    "        ]\n",
    "\n",
    "categorical_variables = ['Prim_Damage','Make','Model'\n",
    "]\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#categories for all categorical variables \n",
    "\n",
    "# categorial_variables = ['Prim_Damage'\n",
    "#                         ,'Sec_Damage'\n",
    "#                         ,'Make'\n",
    "#                         ,'Model' # - to many categories >2000\n",
    "#\n",
    "                        #  ]  \n",
    "out_df[categorical_variables]  =out_df[categorical_variables].astype('category')\n",
    "\n",
    "print(out_df[categorical_variables].describe().to_string(),'\\n\\n')\n",
    "\n",
    "cat_df = {}\n",
    "for col in categorical_variables:\n",
    "    print(col, ' - ', out_df[col].cat.categories.tolist(),'\\n')\n",
    "    cat_df[col] = out_df[col].cat.categories.tolist()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_year_count = out_df['Model_Year'].value_counts()\n",
    "print('total count:',out_df.shape[0],'\\ntotal count (model_year >= 2000):', out_df.loc[out_df.Model_Year >=2000].shape[0])\n",
    "model_year_count.head(20)Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df_describe = out_df[numerical_fields].describe(percentiles =[.01,.25, .5, .75,.99]).T\n",
    "\n",
    "out_df_describe.to_csv('csv/summary_statistics.csv')\n",
    "\n",
    "out_df_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Analysis\n",
    "corr_table = out_df[numerical_fields].corr()\n",
    "\n",
    "corr_table.to_csv('csv/correlation.csv')\n",
    "\n",
    "corr_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_fields_bins = {'Actual_Cash_Value' : [],\n",
    "'Price_Sold_or_Highest_Bid': [],\n",
    "'Repair_Cost': [],\n",
    "'Odometer_Null': [],\n",
    "'Odometer_Replace': [],\n",
    "'Model_Age': }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histomgram for conitnuous variables\n",
    "fig , axs = plt.subplots(len(numerical_fields) ,2, figsize=(12,29),sharex=False)\n",
    "\n",
    "\n",
    "i=0\n",
    "for var in numerical_fields:\n",
    "    # hist_df_lb ,hist_df_hb = out_df[var].loc[lambda x: x <=3e4] , out_df[var].loc[lambda x: x >=1e4]\n",
    "    df_to_print = (out_df[var] // 100).value_counts().to_frame().sort_values(by=var)\n",
    "    df_to_print.to_csv(f'csv/{var}_hist.csv')\n",
    "    print(df_to_print)\n",
    "    # hist_df_lb.plot.hist(bins=10, ax= axs[i][0])\n",
    "    # hist_df_hb.plot.hist(bins=10, ax= axs[i][1])\n",
    "    \n",
    "    # axs[i][0].set(title = var + ' <100k bin')\n",
    "    # axs[i][1].set(title = var + ' >100k bin')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#market size\n",
    "\n",
    "market_size = out_df.groupby(out_df['Auction_Date'].dt.year)['Actual_Cash_Value'].sum()\n",
    "\n",
    "market_size =  market_size / 1000000\n",
    "\n",
    "print(market_size.to_string())\n",
    "plt.figure(figsize=(10,4))\n",
    "ax = market_size.plot.bar()\n",
    "ax.set(title='Market Size', ylabel='$, millions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df[categorical_variables]\n",
    "\n",
    "from DataProcessing.DataStats import get_df_stats\n",
    "\n",
    "get_df_stats(out_df[categorical_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#value distribution for Description \n",
    "\n",
    "desc_val_dist = out_df['Description'].value_counts()\n",
    "\n",
    "desc_val_dist[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#value distribution for Prim Damage \n",
    "\n",
    "prim_val_dist = out_df['Prim_Damage'].value_counts()\n",
    "\n",
    "prim_val_dist[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary statistics conditions on Prim_Damage\n",
    "\n",
    "cond_on_primd = out_df.groupby('Prim_Damage').agg(['mean','var']).stack().sort_index(level=1)\n",
    "cond_on_primd.index = ['{} - {}'.format(j, i) for i, j in cond_on_primd.index]\n",
    "cond_on_primd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary statistics conditions on Make\n",
    "\n",
    "cond_on_make = out_df.groupby('Make').agg(['mean','var']).stack().sort_index(level=1)\n",
    "cond_on_make.index = ['{} - {}'.format(j, i) for i, j in cond_on_make.index]\n",
    "cond_on_make.to_csv('csv/temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(cond_on_make)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_couny = (car_demo_joined.reset_index()\n",
    "             .groupby('County')['index']\n",
    "             .count().sort_values(ascending=False)\n",
    "           )\n",
    "\n",
    "print(by_couny.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = by_couny[:20].plot.bar()\n",
    "ax.set(title='Top 20 Counties by # records',ylabel='# observations');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories for all categorical variables \n",
    "# categorial_variables = ['Prim_Damage'\n",
    "#                         ,'Sec_Damage'\n",
    "#                         ,'Make'\n",
    "#                         ,'Model' # - to many categories >2000\n",
    "#\n",
    "                        #  ]  \n",
    "out_df[categorial_variables]  =out_df[categorial_variables].astype('category')\n",
    "\n",
    "print(out_df[categorial_variables].describe().to_string(),'\\n\\n')\n",
    "\n",
    "cat_df = {}\n",
    "for col in categorial_variables:\n",
    "    print(col, ' - ', out_df[col].cat.categories.tolist(),'\\n')\n",
    "    cat_df[col] = out_df[col].cat.categories.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('sc-research': conda)",
   "language": "python",
   "name": "python37664bitscresearchconda2d2e4dda169440c0a50b32c55aebe5a5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}